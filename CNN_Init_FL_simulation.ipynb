{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 09:59:17.009011: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-07 09:59:17.177439: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746586757.261595     647 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746586757.293997     647 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746586757.426022     647 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746586757.426057     647 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746586757.426058     647 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746586757.426059     647 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-07 09:59:17.441844: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FL_Dataset/file1.csv', 'FL_Dataset/file2.csv', 'FL_Dataset/file3.csv']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import dask.dataframe as dk\n",
    "import tensorflow as tf\n",
    "import io\n",
    "from tensorflow.keras.utils import Sequence, to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "# temp_dir = \"C:/Users/hoang/FileCSV_DACN_2025/ddos_dos_\"\n",
    "\n",
    "input_files = [f\"file{i+1}.csv\" for i in range(3)]\n",
    "temp_dir =  \"FL_Dataset/\"\n",
    "\n",
    "input_files = [temp_dir + output_file for output_file in input_files]\n",
    "print(input_files)\n",
    "df = [dk.read_csv(file) for file in input_files]\n",
    "test_df = dk.read_csv(\"FL_Dataset/test.csv\")\n",
    "# input_zip = \"/mnt/c/Users/hoang/FileCSV_DACN_2025/2Type.zip\"\n",
    "# csv_files = []\n",
    "# with zipfile.ZipFile(input_zip, 'r') as z:\n",
    "#     csv_files = [f for f in z.namelist() if f.endswith('.csv')]\n",
    "# print(csv_files)\n",
    "# df = [dk.read_csv(f'zip://{file}::{input_zip}') for file in csv_files]\n",
    "# print(df[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "ratio_test_all = 0.15\n",
    "\n",
    "from dask_ml.model_selection import train_test_split \n",
    "# Split \n",
    "train_df, val_df = train_test_split(df, test_size=ratio_test_all, random_state=42, shuffle=True)\n",
    "\n",
    "# def dask_to_tf_dataset(dask_df, batch_size, num_classes): \n",
    "#     def generator():\n",
    "#         for batch in dask_df.to_delayed():\n",
    "#             batch=batch.compute()  \n",
    "#             if batch.empty:\n",
    "#                 continue\n",
    "\n",
    "#             X = batch.drop(columns='label').values.astype(np.float32)\n",
    "#             y = batch['label'].values\n",
    "#             y_onehot = to_categorical(y, num_classes=num_classes)  \n",
    "\n",
    "#             num_splits = max(1, len(X) // batch_size)  # Đảm bảo không chia nhỏ quá mức\n",
    "#             X_batches = np.array_split(X, num_splits)\n",
    "#             y_batches = np.array_split(y_onehot, num_splits)\n",
    "\n",
    "#             for X_batch, y_batch in zip(X_batches, y_batches):\n",
    "#                 yield X_batch, y_batch\n",
    "                \n",
    "#     output_signature = ( \n",
    "#         tf.TensorSpec(shape=(None, 46), dtype=tf.float32), \n",
    "#         tf.TensorSpec(shape=(None, 3), dtype=tf.int32),\n",
    "#     )\n",
    "    \n",
    "#     return tf.data.Dataset.from_generator(generator, output_signature=output_signature).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "############## binary ########################\n",
    "# # load từng batch\n",
    "def dask_to_tf_dataset(dask_df, batch_size): \n",
    "    def generator():\n",
    "        for batch in dask_df.to_delayed():\n",
    "            batch = batch.compute()  \n",
    "            if batch.empty:\n",
    "                continue\n",
    "\n",
    "            X = batch.drop(columns='label').values.astype(np.float32)\n",
    "            y = batch['label'].values.astype(np.int32)  # nhị phân: 0 hoặc 1\n",
    "\n",
    "            num_splits = max(1, len(X) // batch_size)\n",
    "            X_batches = np.array_split(X, num_splits)\n",
    "            y_batches = np.array_split(y, num_splits)\n",
    "\n",
    "            for X_batch, y_batch in zip(X_batches, y_batches):\n",
    "                yield X_batch, y_batch\n",
    "\n",
    "    output_signature = ( \n",
    "        tf.TensorSpec(shape=(None, 46), dtype=tf.float32), \n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.int32),  # không phải one-hot nữa\n",
    "    )\n",
    "    \n",
    "    return tf.data.Dataset.from_generator(generator, output_signature=output_signature).prefetch(tf.data.AUTOTUNE)\n",
    "############ binary ###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1746586763.272350     647 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3539 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "# train_df1, test_df1 = df1.random_split([1 - ratio_test_all, ratio_test_all])\n",
    "# train_df2, test_df2 = df2.random_split([1 - ratio_test_all, ratio_test_all])\n",
    "# train_df3, test_df3 = df3.random_split([1 - ratio_test_all, ratio_test_all])\n",
    "train_dfs = []\n",
    "val_dfs = []\n",
    "for dff in df:\n",
    "    train_df, val_df =dff.random_split([1 - ratio_test_all, ratio_test_all])\n",
    "    train_dfs.append(train_df)\n",
    "    val_dfs.append(val_df)\n",
    "   \n",
    "\n",
    "train_gens = [dask_to_tf_dataset(train_df, 512).repeat() for train_df in train_dfs]\n",
    "val_gens = [dask_to_tf_dataset(val_df , 512) for val_df in val_dfs]\n",
    "test_gens = dask_to_tf_dataset(test_df , 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['server_0']\n",
      "['client_0', 'client_1', 'client_2']\n",
      "Train steps:  1693\n",
      "Val steps:  299\n",
      "Test steps:  1992\n",
      "Train steps:  2539\n",
      "Val steps:  449\n",
      "Test steps:  1992\n",
      "Train steps:  2540\n",
      "Val steps:  448\n",
      "Test steps:  1992\n",
      "Agent_Dict:  <client.Client object at 0x7f058e943f40>\n",
      "<server.Server object at 0x7f058e99c550>\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import tenseal as ts\n",
    "#\n",
    "from server import Server\n",
    "from client import Client\n",
    "num_servers = 1\n",
    "num_clients = 3\n",
    "\n",
    "stepsPerEpoch_Clients = [int( np.ceil(train_dfs[index].shape[0] / batch_size)) for index in range(num_clients)]\n",
    "stepsValidate_Clients = [int( np.ceil(val_dfs[index].shape[0] / batch_size)) for index in range(num_clients)]\n",
    "stepsTest_Clients = int( np.ceil(test_df.shape[0] / batch_size))\n",
    "active_servers_list  = ['server_'+str(i)\\\n",
    "                        for i in range(num_servers)]\n",
    "active_clients_list  = ['client_'+str(i)\\\n",
    "                        for i in range(num_clients)]\n",
    "\n",
    "print(active_servers_list)\n",
    "print(active_clients_list)\n",
    "\n",
    "def init_he_context():\n",
    "    \"\"\"Thiết lập context mã hóa đồng hình\"\"\"\n",
    "    context = ts.context(\n",
    "        ts.SCHEME_TYPE.CKKS, # ckks cho số thực, bfv cho int\n",
    "        poly_modulus_degree=8192,\n",
    "        coeff_mod_bit_sizes=[60, 40, 40, 60]\n",
    "    )\n",
    "    context.generate_galois_keys()\n",
    "    context.global_scale = 2**40\n",
    "    return context\n",
    "\n",
    "context = init_he_context()\n",
    "agents_dict= {}\n",
    "serverObjects={}\n",
    "clientObjects={}\n",
    "serverObjects = {server_name: Server(server_name=server_name, \\\n",
    "                        active_clients_list=active_clients_list) \\\n",
    "                        for server_name in active_servers_list}\n",
    "\n",
    "clientObjects = {client_name: Client(client_name, train_gens[clientID], val_gens[clientID], test_gens, \\\n",
    "                        active_clients_list = active_clients_list, he_context=context) \\\n",
    "                        for clientID, client_name in enumerate(active_clients_list)}\n",
    "\n",
    "for index, client_name in enumerate(active_clients_list):\n",
    "    clientObjects[client_name].set_steps_per_epoch(stepsPerEpoch_Clients[index])\n",
    "    clientObjects[client_name].get_steps_per_epoch()\n",
    "    clientObjects[client_name].set_validation_steps(stepsValidate_Clients[index])\n",
    "    clientObjects[client_name].get_validation_steps()\n",
    "    clientObjects[client_name].set_test_steps(stepsTest_Clients)\n",
    "    clientObjects[client_name].get_test_steps()\n",
    "    \n",
    "# lưu dict\n",
    "agents_dict['server'] = serverObjects\n",
    "agents_dict['client'] = clientObjects\n",
    "\n",
    "# init agents_dict vừa tạo vào client, server\n",
    "for agent_name, agent in serverObjects.items():\n",
    "    agent.set_agentsDict(agents_dict=agents_dict)\n",
    "for agent_name, agent in clientObjects.items():\n",
    "    agent.set_agentsDict(agents_dict=agents_dict)\n",
    "\n",
    "client_name = 'client_1'\n",
    "print(\"Agent_Dict: \", agents_dict['client'][client_name])\n",
    "\n",
    "server = agents_dict['server']['server_0']\n",
    "print(server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================== Đang chạy Iteration 1 ======================================\n",
      "Epoch 1/15\n",
      "Epoch 1/15\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1746586776.112082     703 service.cc:152] XLA service 0x7f042d0bd1e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1746586776.112121     703 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 4050 Laptop GPU, Compute Capability 8.9\n",
      "2025-05-07 09:59:36.171728: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1746586776.493952     706 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2025-05-07 09:59:37.754778: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1332', 132 bytes spill stores, 132 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:38.546901: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1332', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:39.285062: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1332', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:39.325927: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854_0', 52 bytes spill stores, 52 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:39.354979: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854', 1064 bytes spill stores, 1064 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:39.355196: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1332', 216 bytes spill stores, 220 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:39.394789: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1332', 216 bytes spill stores, 220 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:39.394829: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:39.396328: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1332', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:39.399440: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1332', 132 bytes spill stores, 132 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:39.648060: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854', 80 bytes spill stores, 80 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:39.651951: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854', 4744 bytes spill stores, 4552 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:39.709860: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854', 7532 bytes spill stores, 7644 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:39.970191: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854', 152 bytes spill stores, 152 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:39.971108: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1332', 132 bytes spill stores, 132 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:40.060989: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854', 180 bytes spill stores, 180 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:40.144025: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854', 80 bytes spill stores, 80 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:40.153102: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854', 80 bytes spill stores, 80 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:40.233675: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854_0', 52 bytes spill stores, 52 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:40.343188: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854', 80 bytes spill stores, 80 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:40.347754: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854', 180 bytes spill stores, 180 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:40.438198: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854', 80 bytes spill stores, 80 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:40.476808: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:40.500630: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854', 180 bytes spill stores, 180 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:40.574409: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854', 152 bytes spill stores, 152 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:40.654365: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:41.024882: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1332', 216 bytes spill stores, 220 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:41.047889: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854_0', 52 bytes spill stores, 52 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:41.048689: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854', 80 bytes spill stores, 80 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:41.226748: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854', 152 bytes spill stores, 152 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:41.227837: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854', 1116 bytes spill stores, 1116 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:41.316943: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854', 804 bytes spill stores, 804 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:41.364220: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854_0', 836 bytes spill stores, 836 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:41.391693: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854', 1064 bytes spill stores, 1064 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:41.453265: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854', 1164 bytes spill stores, 1164 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:41.488433: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854', 804 bytes spill stores, 804 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:41.511760: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854_0', 836 bytes spill stores, 836 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:41.533508: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854', 1164 bytes spill stores, 1164 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:41.536036: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854', 804 bytes spill stores, 804 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:41.550845: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854', 1080 bytes spill stores, 1080 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:41.574133: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854', 1116 bytes spill stores, 1116 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:41.593935: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854', 1080 bytes spill stores, 1080 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:41.611181: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854', 1080 bytes spill stores, 1080 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:41.648073: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854', 1164 bytes spill stores, 1164 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:41.765018: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854_0', 836 bytes spill stores, 836 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:41.765067: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854', 1116 bytes spill stores, 1116 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:41.877111: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854', 1064 bytes spill stores, 1064 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:42.068766: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854', 4744 bytes spill stores, 4552 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:42.092940: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854', 4744 bytes spill stores, 4552 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:42.171753: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854', 7532 bytes spill stores, 7644 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:42.262314: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854', 7532 bytes spill stores, 7644 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:42.695428: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2025-05-07 09:59:42.902168: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2025-05-07 09:59:42.981833: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2025-05-07 09:59:43.062865: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2025-05-07 09:59:43.143576: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2025-05-07 09:59:43.226819: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2025-05-07 09:59:43.309076: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2025-05-07 09:59:43.387765: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2025-05-07 09:59:43.470602: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2025-05-07 09:59:43.554361: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2025-05-07 09:59:43.636712: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2025-05-07 09:59:43.719044: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2025-05-07 09:59:43.722053: W external/local_xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1369] AutotunerUtil::AddResult already existed: <key model='CUDA: 8.9, Cores: 20, GPU clock: 2.055 GHz, Memory bandwidth: 192 GB/s, L2 cache: 24 MB', hlo='{\n",
      "  tmp_0 = f32[513,128]{1,0} parameter(0)\n",
      "  tmp_1 = f32[128]{0} parameter(1)\n",
      "  tmp_2 = f32[513,128]{1,0} broadcast(f32[128]{0} tmp_1), dimensions={1}\n",
      "  tmp_3 = f32[513,128]{1,0} add(f32[513,128]{1,0} tmp_0, f32[513,128]{1,0} tmp_2)\n",
      "  tmp_4 = f32[] constant(0)\n",
      "  tmp_5 = f32[513,128]{1,0} broadcast(f32[] tmp_4), dimensions={}\n",
      "  tmp_6 = f32[513,128]{1,0} maximum(f32[513,128]{1,0} tmp_3, f32[513,128]{1,0} tmp_5)\n",
      "  tmp_7 = f32[513,64]{1,0} parameter(2)\n",
      "  ROOT tmp_8 = f32[128,64]{1,0} dot(f32[513,128]{1,0} tmp_6, f32[513,64]{1,0} tmp_7), lhs_contracting_dims={0}, rhs_contracting_dims={0}, frontend_attributes={grad_x=\"false\",grad_y=\"true\"}\n",
      "}'>\n",
      "2025-05-07 09:59:43.801726: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2025-05-07 09:59:43.882250: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2025-05-07 09:59:43.962327: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2025-05-07 09:59:44.044585: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2025-05-07 09:59:44.126985: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2025-05-07 09:59:44.208465: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2025-05-07 09:59:44.292104: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2025-05-07 09:59:44.377861: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2025-05-07 09:59:44.465670: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2025-05-07 09:59:44.549320: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2025-05-07 09:59:44.560879: W external/local_xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1369] AutotunerUtil::AddResult already existed: <key model='CUDA: 8.9, Cores: 20, GPU clock: 2.055 GHz, Memory bandwidth: 192 GB/s, L2 cache: 24 MB', hlo='{\n",
      "  tmp_0 = f32[513,128]{1,0} parameter(0)\n",
      "  tmp_1 = f32[128]{0} parameter(1)\n",
      "  tmp_2 = f32[513,128]{1,0} broadcast(f32[128]{0} tmp_1), dimensions={1}\n",
      "  tmp_3 = f32[513,128]{1,0} add(f32[513,128]{1,0} tmp_0, f32[513,128]{1,0} tmp_2)\n",
      "  tmp_4 = f32[] constant(0)\n",
      "  tmp_5 = f32[513,128]{1,0} broadcast(f32[] tmp_4), dimensions={}\n",
      "  tmp_6 = f32[513,128]{1,0} maximum(f32[513,128]{1,0} tmp_3, f32[513,128]{1,0} tmp_5)\n",
      "  tmp_7 = f32[128,64]{1,0} parameter(2)\n",
      "  ROOT tmp_8 = f32[513,64]{1,0} dot(f32[513,128]{1,0} tmp_6, f32[128,64]{1,0} tmp_7), lhs_contracting_dims={1}, rhs_contracting_dims={0}, frontend_attributes={grad_x=\"false\",grad_y=\"false\"}\n",
      "}'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m   1/2540\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9:14:00\u001b[0m 13s/step - accuracy: 0.2977 - loss: 1.3202"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1746586784.618721     706 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 311/2540\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 16ms/step - accuracy: 0.9856 - loss: 0.0276"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 09:59:49.639941: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1332', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 331/1693\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - accuracy: 0.9823 - loss: 0.0583"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 09:59:50.044951: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854', 136 bytes spill stores, 136 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 366/2540\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 16ms/step - accuracy: 0.9875 - loss: 0.0241"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 09:59:50.286237: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1332', 112 bytes spill stores, 112 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:50.343891: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1332', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:50.404922: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854', 160 bytes spill stores, 160 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:50.454025: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854', 280 bytes spill stores, 280 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 372/2540\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 16ms/step - accuracy: 0.9876 - loss: 0.0237"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 09:59:50.522558: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854_0', 388 bytes spill stores, 928 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:50.564772: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854', 84 bytes spill stores, 84 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:50.710788: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854_0', 824 bytes spill stores, 824 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:50.872426: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854', 4284 bytes spill stores, 4316 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 381/2540\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 17ms/step - accuracy: 0.9879 - loss: 0.0233"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 09:59:51.175934: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1332', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 429/2540\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 17ms/step - accuracy: 0.9891 - loss: 0.0210"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 09:59:51.667097: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854', 280 bytes spill stores, 280 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:51.833719: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1332', 112 bytes spill stores, 112 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:51.860332: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1332', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 469/2540\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 16ms/step - accuracy: 0.9899 - loss: 0.0195"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 09:59:51.901072: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854', 136 bytes spill stores, 136 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:52.034580: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854', 84 bytes spill stores, 84 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:52.075025: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854', 160 bytes spill stores, 160 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 508/2540\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 15ms/step - accuracy: 0.9905 - loss: 0.0182"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 09:59:52.141510: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854_0', 824 bytes spill stores, 824 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:52.156575: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854_0', 388 bytes spill stores, 928 bytes spill loads\n",
      "\n",
      "2025-05-07 09:59:52.304634: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1854', 4284 bytes spill stores, 4316 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 540/2540\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 14ms/step - accuracy: 0.9910 - loss: 0.0173"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 09:59:52.348473: W external/local_xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1369] AutotunerUtil::AddResult already existed: <key model='CUDA: 8.9, Cores: 20, GPU clock: 2.055 GHz, Memory bandwidth: 192 GB/s, L2 cache: 24 MB', hlo='{\n",
      "  tmp_0 = f32[512,128]{1,0} parameter(0)\n",
      "  tmp_1 = f32[128]{0} parameter(1)\n",
      "  tmp_2 = f32[512,128]{1,0} broadcast(f32[128]{0} tmp_1), dimensions={1}\n",
      "  tmp_3 = f32[512,128]{1,0} add(f32[512,128]{1,0} tmp_0, f32[512,128]{1,0} tmp_2)\n",
      "  tmp_4 = f32[] constant(0)\n",
      "  tmp_5 = f32[512,128]{1,0} broadcast(f32[] tmp_4), dimensions={}\n",
      "  tmp_6 = f32[512,128]{1,0} maximum(f32[512,128]{1,0} tmp_3, f32[512,128]{1,0} tmp_5)\n",
      "  tmp_7 = f32[128,64]{1,0} parameter(2)\n",
      "  ROOT tmp_8 = f32[512,64]{1,0} dot(f32[512,128]{1,0} tmp_6, f32[128,64]{1,0} tmp_7), lhs_contracting_dims={1}, rhs_contracting_dims={0}, frontend_attributes={grad_x=\"false\",grad_y=\"false\"}\n",
      "}'>\n",
      "2025-05-07 09:59:52.381699: W external/local_xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1369] AutotunerUtil::AddResult already existed: <key model='CUDA: 8.9, Cores: 20, GPU clock: 2.055 GHz, Memory bandwidth: 192 GB/s, L2 cache: 24 MB', hlo='{\n",
      "  tmp_0 = f32[512,128]{1,0} parameter(0)\n",
      "  tmp_1 = f32[128]{0} parameter(1)\n",
      "  tmp_2 = f32[512,128]{1,0} broadcast(f32[128]{0} tmp_1), dimensions={1}\n",
      "  tmp_3 = f32[512,128]{1,0} add(f32[512,128]{1,0} tmp_0, f32[512,128]{1,0} tmp_2)\n",
      "  tmp_4 = f32[] constant(0)\n",
      "  tmp_5 = f32[512,128]{1,0} broadcast(f32[] tmp_4), dimensions={}\n",
      "  tmp_6 = f32[512,128]{1,0} maximum(f32[512,128]{1,0} tmp_3, f32[512,128]{1,0} tmp_5)\n",
      "  tmp_7 = f32[512,64]{1,0} parameter(2)\n",
      "  ROOT tmp_8 = f32[128,64]{1,0} dot(f32[512,128]{1,0} tmp_6, f32[512,64]{1,0} tmp_7), lhs_contracting_dims={0}, rhs_contracting_dims={0}, frontend_attributes={grad_x=\"false\",grad_y=\"true\"}\n",
      "}'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2478/2540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9971 - loss: 0.0072"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:00:14.470290: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_101', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2025-05-07 10:00:14.588383: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_101', 132 bytes spill stores, 132 bytes spill loads\n",
      "\n",
      "2025-05-07 10:00:14.604510: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_101', 216 bytes spill stores, 220 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2535/2540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9971 - loss: 0.0073"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:00:15.946201: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_101', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2025-05-07 10:00:16.024827: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_101', 216 bytes spill stores, 220 bytes spill loads\n",
      "\n",
      "2025-05-07 10:00:16.031827: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_101', 132 bytes spill stores, 132 bytes spill loads\n",
      "\n",
      "2025-05-07 10:00:16.790263: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_101', 132 bytes spill stores, 132 bytes spill loads\n",
      "\n",
      "2025-05-07 10:00:16.911581: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_101', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2025-05-07 10:00:16.936123: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_101', 216 bytes spill stores, 220 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2536/2539\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9992 - loss: 0.0027"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:00:21.484505: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_101', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2025-05-07 10:00:21.681930: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_101', 216 bytes spill stores, 220 bytes spill loads\n",
      "\n",
      "2025-05-07 10:00:22.049216: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_101', 132 bytes spill stores, 132 bytes spill loads\n",
      "\n",
      "2025-05-07 10:00:22.108667: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_101', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2025-05-07 10:00:22.174507: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_101', 132 bytes spill stores, 132 bytes spill loads\n",
      "\n",
      "2025-05-07 10:00:22.307546: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_101', 216 bytes spill stores, 220 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 21ms/step - accuracy: 0.9951 - loss: 0.0175 - val_accuracy: 0.8915 - val_loss: 1.4673\n",
      "Epoch 2/15\n",
      "\u001b[1m  26/1693\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.9985 - loss: 0.0124"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:00:23.370438: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "2025-05-07 10:00:23.370504: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_4]]\n",
      "2025-05-07 10:00:23.370517: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "2025-05-07 10:00:23.370587: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n",
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  86/1693\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.9992 - loss: 0.0059"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:00:23.795416: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_101', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2025-05-07 10:00:23.947460: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_101', 216 bytes spill stores, 220 bytes spill loads\n",
      "\n",
      "2025-05-07 10:00:23.976602: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_101', 132 bytes spill stores, 132 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 106/1693\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.9993 - loss: 0.0051"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:00:24.107642: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2540/2540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 17ms/step - accuracy: 0.9971 - loss: 0.0074 - val_accuracy: 0.8929 - val_loss: 1.6423\n",
      "Epoch 2/15\n",
      "\u001b[1m  20/2540\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 9ms/step - accuracy: 0.9897 - loss: 0.0319"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:00:27.530292: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_4]]\n",
      "2025-05-07 10:00:27.530319: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "2025-05-07 10:00:27.530336: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n",
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 827/1693\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.9999 - loss: 0.0011"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:00:29.839242: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_101', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2025-05-07 10:00:30.004552: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_101', 132 bytes spill stores, 132 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 301/2540\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 9ms/step - accuracy: 0.9985 - loss: 0.0198"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:00:30.164732: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_101', 216 bytes spill stores, 220 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 306/2540\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 10ms/step - accuracy: 0.9986 - loss: 0.0197"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:00:30.387145: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2539/2539\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 18ms/step - accuracy: 0.9992 - loss: 0.0028 - val_accuracy: 0.8925 - val_loss: 1.9674\n",
      "Epoch 2/15\n",
      "\u001b[1m1007/1693\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 9.2200e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:00:31.908099: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n",
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 13ms/step - accuracy: 0.9993 - loss: 0.0063 - val_accuracy: 0.8917 - val_loss: 0.8949\n",
      "Epoch 3/15\n",
      "\u001b[1m1205/2539\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 1.4427e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n",
      "2025-05-07 10:00:44.776320: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_4]]\n",
      "2025-05-07 10:00:44.776351: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "2025-05-07 10:00:44.776368: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2540/2540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 14ms/step - accuracy: 0.9992 - loss: 0.0222 - val_accuracy: 0.8929 - val_loss: 1.9105\n",
      "Epoch 3/15\n",
      "\u001b[1m  26/2540\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 6ms/step - accuracy: 0.9997 - loss: 0.0129"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n",
      "2025-05-07 10:01:02.020524: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "2025-05-07 10:01:02.020567: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2539/2539\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 13ms/step - accuracy: 0.9994 - loss: 0.0052 - val_accuracy: 0.8921 - val_loss: 4.4369\n",
      "Epoch 3/15\n",
      "\u001b[1m  20/2539\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 8ms/step - accuracy: 0.9997 - loss: 0.0177"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:01:05.589054: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n",
      "2025-05-07 10:01:05.589095: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 13ms/step - accuracy: 0.9992 - loss: 0.0074 - val_accuracy: 0.8914 - val_loss: 2.2266\n",
      "Epoch 4/15\n",
      "\u001b[1m 662/2540\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.9999 - loss: 0.0024"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n",
      "2025-05-07 10:01:06.530450: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "2025-05-07 10:01:06.530500: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 14ms/step - accuracy: 0.9993 - loss: 0.0097 - val_accuracy: 0.8917 - val_loss: 1.3753\n",
      "Epoch 5/15\n",
      "\u001b[1m2434/2539\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9994 - loss: 0.0078"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:01:30.690584: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_4]]\n",
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n",
      "2025-05-07 10:01:30.690621: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "2025-05-07 10:01:30.690643: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2540/2540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 13ms/step - accuracy: 0.9994 - loss: 0.0082 - val_accuracy: 0.8929 - val_loss: 1.7644\n",
      "Epoch 4/15\n",
      "\u001b[1m  18/2540\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 9ms/step - accuracy: 0.9997 - loss: 0.0326 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n",
      "2025-05-07 10:01:33.813767: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "2025-05-07 10:01:33.813812: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2539/2539\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 13ms/step - accuracy: 0.9993 - loss: 0.0086 - val_accuracy: 0.8925 - val_loss: 2.6224\n",
      "Epoch 4/15\n",
      "\u001b[1m  15/2539\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 12ms/step - accuracy: 0.9998 - loss: 0.0025"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n",
      "2025-05-07 10:01:38.342431: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "2025-05-07 10:01:38.342469: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 13ms/step - accuracy: 0.9995 - loss: 0.0038 - val_accuracy: 0.8917 - val_loss: 2.0651\n",
      "\u001b[1m1781/2540\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.9999 - loss: 0.0050Epoch 6/15\n",
      "\u001b[1m  15/1693\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.9991 - loss: 0.0392"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2540/2540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 13ms/step - accuracy: 0.9994 - loss: 0.0146 - val_accuracy: 0.8929 - val_loss: 2.9527\n",
      "Epoch 5/15\n",
      "\u001b[1m  16/2540\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - accuracy: 0.9995 - loss: 0.0564"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2539/2539\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 13ms/step - accuracy: 0.9996 - loss: 0.0029 - val_accuracy: 0.8925 - val_loss: 2.8752\n",
      "Epoch 5/15\n",
      "\u001b[1m 582/2540\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.9999 - loss: 0.0065"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n",
      "2025-05-07 10:02:12.090118: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "2025-05-07 10:02:12.090167: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 13ms/step - accuracy: 0.9995 - loss: 0.0066 - val_accuracy: 0.9434 - val_loss: 0.2957\n",
      "Epoch 7/15\n",
      "\u001b[1m 206/2539\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 9ms/step - accuracy: 0.9998 - loss: 0.0091"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:02:13.877788: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n",
      "2025-05-07 10:02:13.877884: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 15ms/step - accuracy: 0.9996 - loss: 0.0048 - val_accuracy: 0.8917 - val_loss: 1.0281\n",
      "\u001b[1m2499/2539\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9997 - loss: 0.0050Epoch 8/15\n",
      "\u001b[1m  20/1693\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.0339e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2540/2540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 13ms/step - accuracy: 0.9994 - loss: 0.0116 - val_accuracy: 0.8929 - val_loss: 1.3856\n",
      "\u001b[1m 318/1693\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.9999 - loss: 0.0028Epoch 6/15\n",
      "\u001b[1m  17/2540\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0156"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:02:40.652801: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_2]]\n",
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2539/2539\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 13ms/step - accuracy: 0.9997 - loss: 0.0051 - val_accuracy: 0.8925 - val_loss: 3.5051\n",
      "Epoch 6/15\n",
      "\u001b[1m 830/1693\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 0.0018"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:02:45.484224: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n",
      "2025-05-07 10:02:45.484272: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 13ms/step - accuracy: 0.9996 - loss: 0.0062 - val_accuracy: 0.9362 - val_loss: 0.4532\n",
      "\u001b[1m1379/2539\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 11ms/step - accuracy: 0.9999 - loss: 0.0056Epoch 9/15\n",
      "\u001b[1m1915/2540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m6s\u001b[0m 11ms/step - accuracy: 0.9999 - loss: 0.0137"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n",
      "2025-05-07 10:03:00.938044: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "2025-05-07 10:03:00.938087: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2540/2540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 14ms/step - accuracy: 0.9995 - loss: 0.0204 - val_accuracy: 0.8929 - val_loss: 1.4354\n",
      "Epoch 7/15\n",
      "\u001b[1m  17/2540\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0027    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:03:15.335109: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2539/2539\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 14ms/step - accuracy: 0.9996 - loss: 0.0074 - val_accuracy: 0.8924 - val_loss: 1.3752\n",
      "Epoch 7/15\n",
      "\u001b[1m 619/2540\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.9999 - loss: 0.0118"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:03:20.127690: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n",
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 13ms/step - accuracy: 0.9996 - loss: 0.0047 - val_accuracy: 0.9371 - val_loss: 0.2336\n",
      "Epoch 10/15\n",
      "\u001b[1m 865/2540\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 8ms/step - accuracy: 0.9999 - loss: 0.0096"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 15ms/step - accuracy: 0.9996 - loss: 0.0046 - val_accuracy: 0.8917 - val_loss: 0.6743\n",
      "Epoch 11/15\n",
      "\u001b[1m  28/1693\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.9999 - loss: 0.0024"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:03:47.178400: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "2025-05-07 10:03:47.178448: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n",
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2540/2540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 13ms/step - accuracy: 0.9996 - loss: 0.0081 - val_accuracy: 0.8929 - val_loss: 1.4566\n",
      "\u001b[1m 203/1693\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.9999 - loss: 0.0050Epoch 8/15\n",
      "\u001b[1m  19/2540\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 0.0898"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:03:48.506526: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n",
      "2025-05-07 10:03:48.506613: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2539/2539\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 13ms/step - accuracy: 0.9996 - loss: 0.0062 - val_accuracy: 0.8923 - val_loss: 2.6785\n",
      "Epoch 8/15\n",
      "\u001b[1m 757/1693\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 0.0041"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 13ms/step - accuracy: 0.9997 - loss: 0.0041 - val_accuracy: 0.8917 - val_loss: 1.4557\n",
      "Epoch 12/15\n",
      "\u001b[1m2038/2540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.9999 - loss: 0.0437"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:04:09.543515: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n",
      "2025-05-07 10:04:09.543559: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2540/2540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 13ms/step - accuracy: 0.9996 - loss: 0.0428 - val_accuracy: 0.8929 - val_loss: 0.5169\n",
      "Epoch 9/15\n",
      "\u001b[1m  19/2540\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 9ms/step - accuracy: 0.9998 - loss: 0.0436"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:04:22.004697: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n",
      "2025-05-07 10:04:22.004734: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2539/2539\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 13ms/step - accuracy: 0.9994 - loss: 0.0094 - val_accuracy: 0.8924 - val_loss: 2.7666\n",
      "Epoch 9/15\n",
      "\u001b[1m 679/2540\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 0.0196"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:04:27.790032: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n",
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 13ms/step - accuracy: 0.9997 - loss: 0.0034 - val_accuracy: 0.9187 - val_loss: 0.5961\n",
      "Epoch 13/15\n",
      "\u001b[1m  15/1693\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.9999 - loss: 0.0121"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2540/2540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 13ms/step - accuracy: 0.9995 - loss: 0.0148 - val_accuracy: 0.9223 - val_loss: 0.3272\n",
      "Epoch 10/15\n",
      "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 14ms/step - accuracy: 0.9996 - loss: 0.0066 - val_accuracy: 0.9010 - val_loss: 0.4000\n",
      "Epoch 14/15\n",
      "\u001b[1m  18/2540\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0020"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:04:55.424380: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n",
      "2025-05-07 10:04:55.424423: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2539/2539\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 13ms/step - accuracy: 0.9990 - loss: 0.0164 - val_accuracy: 0.8923 - val_loss: 1.5038\n",
      "Epoch 10/15\n",
      "\u001b[1m 605/2540\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 0.0012"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:05:00.904985: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n",
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 14ms/step - accuracy: 0.9996 - loss: 0.0052 - val_accuracy: 0.9998 - val_loss: 0.0109\n",
      "\u001b[1m2233/2540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.9998 - loss: 0.0015Epoch 15/15\n",
      "\u001b[1m  14/1693\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.9995 - loss: 0.0136"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:05:18.848047: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_4]]\n",
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n",
      "2025-05-07 10:05:18.848103: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "2025-05-07 10:05:18.848145: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2540/2540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 13ms/step - accuracy: 0.9997 - loss: 0.0020 - val_accuracy: 0.9402 - val_loss: 0.1452\n",
      "Epoch 11/15\n",
      "\u001b[1m1012/1693\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - accuracy: 0.9999 - loss: 0.0034"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:05:28.744306: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2539/2539\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 13ms/step - accuracy: 0.9980 - loss: 0.0206 - val_accuracy: 0.8924 - val_loss: 0.2812\n",
      "Epoch 11/15\n",
      "\u001b[1m  15/2539\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 12ms/step - accuracy: 0.9993 - loss: 0.1141"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:05:34.610932: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n",
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 13ms/step - accuracy: 0.9996 - loss: 0.0056 - val_accuracy: 0.8915 - val_loss: 1.1344\n",
      "\u001b[1m1235/2540\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 0.0012"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:05:40.096048: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n",
      "2025-05-07 10:05:40.096133: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1244/2540\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 0.0012client_0Come end!\n",
      "\u001b[1m2540/2540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - accuracy: 0.9997 - loss: 0.0019 - val_accuracy: 0.8930 - val_loss: 0.3916\n",
      "Epoch 12/15\n",
      "\u001b[1m  23/2540\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 8ms/step - accuracy: 0.9999 - loss: 0.0016"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:05:56.975651: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "2025-05-07 10:05:56.975741: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n",
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2539/2539\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 10ms/step - accuracy: 0.9920 - loss: 0.0318 - val_accuracy: 0.8925 - val_loss: 0.4105\n",
      "Epoch 12/15\n",
      "\u001b[1m  22/2539\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 8ms/step - accuracy: 0.9991 - loss: 0.3623"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:06:00.899038: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n",
      "2025-05-07 10:06:00.899086: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2540/2540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 9ms/step - accuracy: 0.9996 - loss: 0.0023 - val_accuracy: 0.8930 - val_loss: 0.5975\n",
      "Epoch 13/15\n",
      "\u001b[1m  30/2540\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - accuracy: 0.9998 - loss: 0.0055"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:06:20.898086: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n",
      "2025-05-07 10:06:20.898125: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2539/2539\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 10ms/step - accuracy: 0.9985 - loss: 0.0280 - val_accuracy: 0.8925 - val_loss: 0.8386\n",
      "Epoch 13/15\n",
      "\u001b[1m  21/2539\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 8ms/step - accuracy: 0.9999 - loss: 0.0042"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:06:25.232695: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "2025-05-07 10:06:25.232738: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n",
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2540/2540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 10ms/step - accuracy: 0.9996 - loss: 0.0044 - val_accuracy: 0.8930 - val_loss: 0.2996\n",
      "Epoch 14/15\n",
      "\u001b[1m  25/2540\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.9999 - loss: 7.2758e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:06:45.503146: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n",
      "2025-05-07 10:06:45.503189: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2539/2539\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 10ms/step - accuracy: 0.9993 - loss: 0.0067 - val_accuracy: 0.8925 - val_loss: 2.9229\n",
      "Epoch 14/15\n",
      "\u001b[1m 871/2540\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 3.6803e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:06:50.801531: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n",
      "2025-05-07 10:06:50.801577: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2540/2540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 10ms/step - accuracy: 0.9997 - loss: 0.0020 - val_accuracy: 0.9037 - val_loss: 0.2134\n",
      "Epoch 15/15\n",
      "\u001b[1m  31/2540\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0024"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:07:09.822548: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "2025-05-07 10:07:09.822590: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n",
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2539/2539\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 9ms/step - accuracy: 0.9995 - loss: 0.0092 - val_accuracy: 0.8924 - val_loss: 0.5478\n",
      "Epoch 15/15\n",
      "\u001b[1m  20/2539\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 8ms/step - accuracy: 0.9995 - loss: 0.0135"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n",
      "2025-05-07 10:07:14.775302: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "2025-05-07 10:07:14.775342: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2540/2540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 9ms/step - accuracy: 0.9996 - loss: 0.0032 - val_accuracy: 0.8929 - val_loss: 0.2832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:07:33.666776: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n",
      "2025-05-07 10:07:33.666826: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client_2Come end!\n",
      "\u001b[1m2539/2539\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 9ms/step - accuracy: 0.9991 - loss: 0.0079 - val_accuracy: 0.9819 - val_loss: 0.0819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:07:37.395713: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n",
      "2025-05-07 10:07:37.395752: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client_1Come end!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/losses/losses.py:33: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
      "  return self.fn(y_true, y_pred, **self._fn_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.9977 - loss: 0.0000e+00\n",
      "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.9868 - loss: 0.0000e+00\n",
      "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.9870 - loss: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:07:52.987364: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n",
      "2025-05-07 10:07:52.987400: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n",
      "2025-05-07 10:07:52.998959: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "2025-05-07 10:07:53.035802: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "2025-05-07 10:07:53.035842: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n",
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/losses/losses.py:33: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
      "  return self.fn(y_true, y_pred, **self._fn_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.9870 - loss: 0.0000e+00\n",
      "Arguments:  Performance Metrics for client_2 on iteration 1 \n",
      "------------------------------------------- \n",
      "local accuracy: 0.8917753100395203 \n",
      "global accuracy: 0.8917753100395203 \n",
      "local compute time: 0:08:03.702666 \n",
      "Simulated time to receive global weights: 0:08:07.977262 \n",
      " \n",
      "\n",
      "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.9870 - loss: 0.0000e+00\n",
      "Arguments:  Performance Metrics for client_1 on iteration 1 \n",
      "------------------------------------------- \n",
      "local accuracy: 0.9822093844413757 \n",
      "global accuracy: 0.8917625546455383 \n",
      "local compute time: 0:08:07.339171 \n",
      "Simulated time to receive global weights: 0:08:07.977262 \n",
      " \n",
      "\n",
      "\u001b[1m1992/1992\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.9869 - loss: 0.0000e+00\n",
      "Arguments:  Performance Metrics for client_0 on iteration 1 \n",
      "------------------------------------------- \n",
      "local accuracy: 0.8916056752204895 \n",
      "global accuracy: 0.8917046785354614 \n",
      "local compute time: 0:06:10.259573 \n",
      "Simulated time to receive global weights: 0:08:07.977262 \n",
      " \n",
      "\n",
      "[client_0] :Simulated time for client set() to finish iteration 1: 0:16:16.925550\n",
      "\n",
      "[client_1] :Simulated time for client set() to finish iteration 1: 0:16:16.925550\n",
      "\n",
      "[client_2] :Simulated time for client set() to finish iteration 1: 0:16:16.925550\n",
      "\n",
      "====================================== Kết thúc Iteration 1 ======================================\n",
      "====================================== Đang chạy Iteration 2 ======================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:08:05.666458: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "2025-05-07 10:08:05.666498: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n",
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n",
      "2025-05-07 10:08:05.685812: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "2025-05-07 10:08:05.685898: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n",
      "2025-05-07 10:08:05.749796: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "2025-05-07 10:08:05.749838: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Come here!\n",
      "Epoch 1/15\n",
      "2 Come here!\n",
      "Epoch 1/15\n",
      "2 Come here!\n",
      "Epoch 1/15\n",
      "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 18ms/step - accuracy: 0.9995 - loss: 0.0031 - val_accuracy: 0.8980 - val_loss: 0.8317\n",
      "Epoch 2/15\n",
      "\u001b[1m  18/1693\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 10ms/step - accuracy: 0.9994 - loss: 0.0148"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:08:47.464026: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "2025-05-07 10:08:47.464102: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n",
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2540/2540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 15ms/step - accuracy: 0.9994 - loss: 0.0042 - val_accuracy: 0.8929 - val_loss: 1.7070\n",
      "Epoch 2/15\n",
      "\u001b[1m1005/1693\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0013"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:08:54.602686: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n",
      "2025-05-07 10:08:54.602729: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2539/2539\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 16ms/step - accuracy: 0.9996 - loss: 0.0027 - val_accuracy: 0.8925 - val_loss: 1.2837\n",
      "\u001b[1m1295/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0012Epoch 2/15\n",
      "\u001b[1m 319/2540\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 0.0016"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:08:57.331104: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "2025-05-07 10:08:57.331178: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n",
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.9995 - loss: 0.0072 - val_accuracy: 0.8915 - val_loss: 1.3729\n",
      "Epoch 3/15\n",
      "\u001b[1m 907/2539\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0019"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:09:06.657746: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n",
      "2025-05-07 10:09:06.657784: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2540/2540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 14ms/step - accuracy: 0.9994 - loss: 0.0086 - val_accuracy: 0.8929 - val_loss: 3.1648\n",
      "Epoch 3/15\n",
      "\u001b[1m  20/2540\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 8ms/step - accuracy: 0.9997 - loss: 0.0103"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:09:29.610317: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n",
      "2025-05-07 10:09:29.610367: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 14ms/step - accuracy: 0.9994 - loss: 0.0108 - val_accuracy: 0.8917 - val_loss: 1.2893\n",
      "Epoch 4/15\n",
      "\u001b[1m  19/1693\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 9ms/step - accuracy: 0.9997 - loss: 0.0199"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2539/2539\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 14ms/step - accuracy: 0.9996 - loss: 0.0059 - val_accuracy: 0.8925 - val_loss: 2.8422\n",
      "Epoch 3/15\n",
      "\u001b[1m 383/2540\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 0.0034"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:09:32.858708: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n",
      "2025-05-07 10:09:32.858751: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 14ms/step - accuracy: 0.9996 - loss: 0.0072 - val_accuracy: 0.8917 - val_loss: 0.9606\n",
      "Epoch 5/15\n",
      "\u001b[1m1924/2539\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m6s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 6.7202e-05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2540/2540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 13ms/step - accuracy: 0.9992 - loss: 0.0088 - val_accuracy: 0.8929 - val_loss: 2.6224\n",
      "Epoch 4/15\n",
      "\u001b[1m1070/1693\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 0.0036"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:10:03.900738: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n",
      "2025-05-07 10:10:03.900780: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2539/2539\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 14ms/step - accuracy: 0.9997 - loss: 0.0051 - val_accuracy: 0.8922 - val_loss: 2.2224\n",
      "Epoch 4/15\n",
      "\u001b[1m1493/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9997 - loss: 0.0057"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:10:07.945780: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n",
      "2025-05-07 10:10:07.945819: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - accuracy: 0.9996 - loss: 0.0073 - val_accuracy: 0.8915 - val_loss: 0.7872\n",
      "Epoch 6/15\n",
      "\u001b[1m 756/2539\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 10ms/step - accuracy: 0.9999 - loss: 0.0041"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n",
      "2025-05-07 10:10:15.426307: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "2025-05-07 10:10:15.426335: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2540/2540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 14ms/step - accuracy: 0.9993 - loss: 0.0133 - val_accuracy: 0.8929 - val_loss: 0.8630\n",
      "Epoch 5/15\n",
      "\u001b[1m  20/2540\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 8ms/step - accuracy: 0.9995 - loss: 0.0948"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:10:38.389407: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "2025-05-07 10:10:38.389455: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n",
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 14ms/step - accuracy: 0.9995 - loss: 0.0075 - val_accuracy: 0.8915 - val_loss: 1.0526\n",
      "Epoch 7/15\n",
      "\u001b[1m  20/1693\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0103"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:10:38.930562: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_4]]\n",
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n",
      "2025-05-07 10:10:38.930592: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "2025-05-07 10:10:38.930609: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2539/2539\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 14ms/step - accuracy: 0.9996 - loss: 0.0116 - val_accuracy: 0.8925 - val_loss: 1.9163\n",
      "Epoch 5/15\n",
      "\u001b[1m 380/1693\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 10ms/step - accuracy: 0.9999 - loss: 0.0146"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 14ms/step - accuracy: 0.9993 - loss: 0.0178 - val_accuracy: 0.8917 - val_loss: 1.3070\n",
      "\u001b[1m2249/2540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.9994 - loss: 0.0222Epoch 8/15\n",
      "\u001b[1m  16/1693\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0066"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:11:02.817739: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "2025-05-07 10:11:02.817901: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n",
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2540/2540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 13ms/step - accuracy: 0.9991 - loss: 0.0258 - val_accuracy: 0.8929 - val_loss: 0.5672\n",
      "Epoch 6/15\n",
      "\u001b[1m  19/2540\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0575"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:11:12.589757: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "2025-05-07 10:11:12.589803: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n",
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2539/2539\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 14ms/step - accuracy: 0.9996 - loss: 0.0054 - val_accuracy: 0.8925 - val_loss: 1.0856\n",
      "Epoch 6/15\n",
      "\u001b[1m1519/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9994 - loss: 0.0144"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n",
      "2025-05-07 10:11:16.876538: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - accuracy: 0.9991 - loss: 0.0175 - val_accuracy: 0.8917 - val_loss: 0.9421\n",
      "Epoch 9/15\n",
      "\u001b[1m 688/2539\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 10ms/step - accuracy: 0.9999 - loss: 0.0123"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n",
      "2025-05-07 10:11:23.727551: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "2025-05-07 10:11:23.727595: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2540/2540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 13ms/step - accuracy: 0.9990 - loss: 0.0464 - val_accuracy: 0.8929 - val_loss: 0.7996\n",
      "Epoch 7/15\n",
      "\u001b[1m  19/2540\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0074"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:11:46.348188: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n",
      "2025-05-07 10:11:46.348241: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 14ms/step - accuracy: 0.9990 - loss: 0.0147 - val_accuracy: 0.8915 - val_loss: 0.4157\n",
      "Epoch 10/15\n",
      "\u001b[1m  18/1693\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0399"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:11:47.194583: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n",
      "2025-05-07 10:11:47.194671: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2539/2539\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 14ms/step - accuracy: 0.9991 - loss: 0.0147 - val_accuracy: 0.8925 - val_loss: 2.3191\n",
      "Epoch 7/15\n",
      "\u001b[1m 448/1693\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 10ms/step - accuracy: 0.9999 - loss: 0.0156"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 14ms/step - accuracy: 0.9986 - loss: 0.0246 - val_accuracy: 0.8915 - val_loss: 0.6268\n",
      "Epoch 11/15\n",
      "\u001b[1m  14/1693\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 12ms/step - accuracy: 0.9997 - loss: 0.0447"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:12:11.028180: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n",
      "2025-05-07 10:12:11.028220: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2540/2540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 14ms/step - accuracy: 0.9987 - loss: 0.0110 - val_accuracy: 0.8929 - val_loss: 2.3252\n",
      "Epoch 8/15\n",
      "\u001b[1m  20/2540\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 0.0023"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n",
      "2025-05-07 10:12:20.888246: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "2025-05-07 10:12:20.888288: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2539/2539\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 14ms/step - accuracy: 0.9995 - loss: 0.0122 - val_accuracy: 0.8925 - val_loss: 0.8869\n",
      "Epoch 8/15\n",
      "\u001b[1m   9/2539\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 14ms/step - accuracy: 0.9994 - loss: 0.0766"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 13ms/step - accuracy: 0.9979 - loss: 0.0305 - val_accuracy: 0.8916 - val_loss: 0.4819\n",
      "Epoch 12/15\n",
      "\u001b[1m1235/2540\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 10ms/step - accuracy: 0.9999 - loss: 0.0023"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:12:32.810700: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "2025-05-07 10:12:32.810751: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n",
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2540/2540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 14ms/step - accuracy: 0.9989 - loss: 0.0096 - val_accuracy: 0.8928 - val_loss: 0.9090\n",
      "Epoch 9/15\n",
      "\u001b[1m  15/2540\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 8ms/step - accuracy: 0.9998 - loss: 0.0189"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:12:55.393640: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n",
      "2025-05-07 10:12:55.393684: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 14ms/step - accuracy: 0.9984 - loss: 0.0259 - val_accuracy: 0.8917 - val_loss: 0.6210\n",
      "Epoch 13/15\n",
      "\u001b[1m  17/1693\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 10ms/step - accuracy: 0.9999 - loss: 0.0192"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:12:57.080017: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n",
      "2025-05-07 10:12:57.080066: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2539/2539\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 13ms/step - accuracy: 0.9992 - loss: 0.0258 - val_accuracy: 0.8922 - val_loss: 0.5860\n",
      "Epoch 9/15\n",
      "\u001b[1m 572/2540\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 0.0061"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:13:00.169522: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n",
      "2025-05-07 10:13:00.169564: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1693/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 15ms/step - accuracy: 0.9982 - loss: 0.0206 - val_accuracy: 0.8915 - val_loss: 0.4837\n",
      "Epoch 14/15\n",
      "\u001b[1m1890/2539\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 12ms/step - accuracy: 0.9999 - loss: 0.0213"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:13:22.501552: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "2025-05-07 10:13:22.501878: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n",
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2540/2540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 14ms/step - accuracy: 0.9990 - loss: 0.0151 - val_accuracy: 0.8929 - val_loss: 1.6318\n",
      "Epoch 10/15\n",
      "\u001b[1m  12/2540\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 3.8672e-04 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:13:31.487903: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15685740104855525371\n",
      "2025-05-07 10:13:31.487955: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17549315412094336176\n",
      "/home/th/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1545/1693\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9986 - loss: 0.0188"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    server.InitLoop()\n",
    "    server.final_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "# tempdirs = [\"D:/DoAnChuyenNganh_Train/client_0_log/11h18p__02-04-2025/\", \"D:/DoAnChuyenNganh_Train/client_1_log/11h18p__02-04-2025/\",  \"D:/DoAnChuyenNganh_Train/client_2_log/11h18p__02-04-2025/\"]\n",
    "timeFolder=\"14h24p__15-04/\"\n",
    "tempdirs = [f\"D:/DoAnChuyenNganh_Train/log/client_{i}_log/\" for i in range(len(active_clients_list))]\n",
    "\n",
    "\n",
    "model_names =[timeFolder+f\"model_{i+1}.keras\" for i in range(5)]\n",
    "print(model_names)\n",
    "models = {}\n",
    "\n",
    "for i, client_name in enumerate(active_clients_list):\n",
    "    models[client_name] = [load_model(tempdirs[i]+model_name) for model_name in model_names]\n",
    "print (models['client_0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "num_batch_test_eachClient  =  []\n",
    "for index, test_df in enumerate(test_dfs):\n",
    "    num_samples_test = test_df.shape[0].compute()\n",
    "    # Tính số batch\n",
    "    num_batches_test = int(np.ceil(num_samples_test / batch_size))\n",
    "    num_batch_test_eachClient.append(num_batches_test)\n",
    "\n",
    "print(\"Num Batch Each Client: \", num_batch_test_eachClient)\n",
    "X_tests = {}\n",
    "Y_tests = {}\n",
    "Y_preds= {}\n",
    "for i, client_name in enumerate(active_clients_list):\n",
    "    Y_tests[client_name]={}\n",
    "    Y_preds[client_name]={}\n",
    "\n",
    "# for i, client_name in enumerate(active_clients_list):\n",
    "#     X_test = []\n",
    "#     y_test = []\n",
    "#     for X_batch, y_batch in test_gens[i].take(num_batch_test_eachClient[i]):\n",
    "#         X_test.extend(X_batch.numpy().flatten())\n",
    "#         y_test.extend(y_batch.numpy().flatten())\n",
    "#         y_pred = []\n",
    "#         for iteration in range(5):    \n",
    "#     # .as_numpy_iterator():\n",
    "#         # # take(12000):\n",
    "#         # X_test_list.append(X_batch.numpy())\n",
    "#         # y_test_list.append(y_batch.numpy())  # .numpy()\n",
    "\n",
    "#         # # Gộp tất cả batch lại\n",
    "#         # X_test = np.concatenate(X_test_list, axis=0)\n",
    "#         # y_test = np.concatenate(y_test_list, axis=0)\n",
    "\n",
    "#         # # Nếu y_test đang ở dạng one-hot, chuyển về dạng số\n",
    "#         # y_test = np.argmax(y_test, axis=1)\n",
    "#             y_pred_pre = models[client_name][iteration].predict(X_batch, verbose=0)\n",
    "#             y_pred.extend((y_pred_pre > 0.5).astype(int).flatten())\n",
    "    \n",
    "#         Y_tests[client_name][iteration] = y_test\n",
    "#         Y_preds[client_name][iteration] = y_pred\n",
    "    \n",
    "# print(Y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Client 0\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1s = []\n",
    "\n",
    "Iterations = [f\"Iteration {index+1}\" for index in range(5)]\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "for X_batch, y_batch in test_gens[i].take(num_batch_test_eachClient[0]):\n",
    "    X_test.append(X_batch.numpy())\n",
    "    y_test.append(y_batch.numpy())\n",
    "X_test = np.concatenate(X_test, axis=0)\n",
    "y_test = np.concatenate(y_test, axis=0)\n",
    "\n",
    "print(len(X_test))\n",
    "for iteration in range(len(Iterations)):\n",
    "    print(models[client_name][iteration])\n",
    "    y_pred_pre = models[client_name][iteration].predict(X_test, verbose=1)\n",
    "    y_pred = (y_pred_pre > 0.5).astype(int).flatten()\n",
    "    \n",
    "    precisions.append(precision_score(y_test, y_pred, average='binary'))\n",
    "    recalls.append(recall_score(y_test, y_pred, average='binary'))\n",
    "    f1s.append(f1_score(y_test, y_pred, average='binary'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision Score: \", precisions)\n",
    "print(\"Recall Score: \", recalls)\n",
    "print(\"F1 Score: \", f1s)  \n",
    "\n",
    "\n",
    "# Vẽ biểu đồ\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(Iterations, precisions, marker='o', linestyle='-', label=\"Precision\", color='blue')\n",
    "plt.plot(Iterations, recalls, marker='s', linestyle='-', label=\"Recall\", color='red')\n",
    "plt.plot(Iterations, f1s, marker='^', linestyle='-', label=\"F1-score\", color='green')\n",
    "\n",
    "# Thêm tiêu đề và nhãn\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Precision, Recall, F1-score over different iterations _ CLIENT 0\")\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)  # Giới hạn từ 0 đến 1\n",
    "plt.grid(True)\n",
    "\n",
    "# Hiển thị đồ thị\n",
    "plt.show()\n",
    "\n",
    "\n",
    "attack_types = ['BenignTraffic', 'DoS&DDoS']\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "    # Vẽ heatmap\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=attack_types, yticklabels=attack_types)\n",
    "plt.yticks(rotation=360)\n",
    "# Thêm nhãn\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix CLIENT 0\")\n",
    "\n",
    "# Hiển thị\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# attack_types = ['BenignTraffic', 'DDoS-ICMP_Flood', 'DDoS-PSHACK_Flood', 'DDoS-RSTFINFlood', 'DDoS-SYN_Flood', \n",
    "#                    'DDoS-SynonymousIP_Flood', 'DDoS-TCP_Flood', 'DDoS-UDP_Flood', 'DoS-SYN_Flood', 'DoS-TCP_Flood', 'DoS-UDP_Flood']\n",
    "\n",
    "metrics = []\n",
    "\n",
    "# Số lượng lớp (10 lớp)\n",
    "num_classes = len(attack_types)\n",
    "\n",
    "# Duyệt từng lớp để tính TP, FP, TN, FN\n",
    "for i in range(num_classes):\n",
    "    TP = cm[i, i]\n",
    "    FP = cm[:, i].sum() - TP\n",
    "    FN = cm[i, :].sum() - TP\n",
    "    TN = cm.sum() - (TP + FP + FN)\n",
    "    \n",
    "    metrics.append([attack_types[i], TP, FP, TN, FN])\n",
    "\n",
    "# Chuyển thành DataFrame\n",
    "df_metrics = pd.DataFrame(metrics, columns=[\"Attack_Types\", \"TP\", \"FP\", \"TN\", \"FN\"])\n",
    "\n",
    "# Vẽ biểu đồ\n",
    "df_metrics.set_index(\"Attack_Types\").plot(kind=\"bar\", figsize=(12, 5), colormap=\"viridis\")\n",
    "\n",
    "# Thêm nhãn\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"TP, FP, TN, FN for Each Class CLIENT 0\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend([\"TP\", \"FP\", \"TN\", \"FN\"])\n",
    "\n",
    "# Hiển thị\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Client 1\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1s = []\n",
    "y_pred = []\n",
    "\n",
    "Iterations = [f\"Iteration {index+1}\" for index in range(5)]\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "print(num_batch_test_eachClient[1])\n",
    "\n",
    "for X_batch, y_batch in test_gens[1].take(num_batch_test_eachClient[1]):\n",
    "    X_test.append(X_batch.numpy())\n",
    "    y_test.append(y_batch.numpy())\n",
    "X_test = np.concatenate(X_test, axis=0)\n",
    "y_test = np.concatenate(y_test, axis=0)\n",
    "\n",
    "for iteration in range(len(Iterations)):\n",
    "    y_pred_pre = models['client_1'][iteration].predict(X_test, verbose=1)\n",
    "    y_pred = (y_pred_pre > 0.5).astype(int).flatten()\n",
    "    \n",
    "    precisions.append(precision_score(y_test, y_pred, average='binary'))\n",
    "    recalls.append(recall_score(y_test, y_pred, average='binary'))\n",
    "    f1s.append(f1_score(y_test, y_pred, average='binary'))\n",
    "    \n",
    "print(\"Precision Score: \", precisions)\n",
    "print(\"Recall Score: \", recalls)\n",
    "print(\"F1 Score: \", f1s)  \n",
    "\n",
    "\n",
    "# Vẽ biểu đồ\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(Iterations, precisions, marker='o', linestyle='-', label=\"Precision\", color='blue')\n",
    "plt.plot(Iterations, recalls, marker='s', linestyle='-', label=\"Recall\", color='red')\n",
    "plt.plot(Iterations, f1s, marker='^', linestyle='-', label=\"F1-score\", color='green')\n",
    "\n",
    "# Thêm tiêu đề và nhãn\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Precision, Recall, F1-score over different iterations _ CLIENT 1\")\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)  # Giới hạn từ 0 đến 1\n",
    "plt.grid(True)\n",
    "\n",
    "# Hiển thị đồ thị\n",
    "plt.show()\n",
    "\n",
    "\n",
    "attack_types = ['BenignTraffic', 'DoS&DDoS']\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "    # Vẽ heatmap\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=attack_types, yticklabels=attack_types)\n",
    "plt.yticks(rotation=360)\n",
    "# Thêm nhãn\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix CLIENT 1\")\n",
    "\n",
    "# Hiển thị\n",
    "plt.show()\n",
    "\n",
    "# attack_types = ['BenignTraffic', 'DDoS-ICMP_Flood', 'DDoS-PSHACK_Flood', 'DDoS-RSTFINFlood', 'DDoS-SYN_Flood', \n",
    "#                    'DDoS-SynonymousIP_Flood', 'DDoS-TCP_Flood', 'DDoS-UDP_Flood', 'DoS-SYN_Flood', 'DoS-TCP_Flood', 'DoS-UDP_Flood']\n",
    "\n",
    "metrics = []\n",
    "\n",
    "# Số lượng lớp (10 lớp)\n",
    "num_classes = len(attack_types)\n",
    "\n",
    "# Duyệt từng lớp để tính TP, FP, TN, FN\n",
    "for i in range(num_classes):\n",
    "    TP = cm[i, i]\n",
    "    FP = cm[:, i].sum() - TP\n",
    "    FN = cm[i, :].sum() - TP\n",
    "    TN = cm.sum() - (TP + FP + FN)\n",
    "    \n",
    "    metrics.append([attack_types[i], TP, FP, TN, FN])\n",
    "\n",
    "# Chuyển thành DataFrame\n",
    "df_metrics = pd.DataFrame(metrics, columns=[\"Attack_Types\", \"TP\", \"FP\", \"TN\", \"FN\"])\n",
    "\n",
    "# Vẽ biểu đồ\n",
    "df_metrics.set_index(\"Attack_Types\").plot(kind=\"bar\", figsize=(12, 5), colormap=\"viridis\")\n",
    "\n",
    "# Thêm nhãn\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"TP, FP, TN, FN for Each Class CLIENT 1\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend([\"TP\", \"FP\", \"TN\", \"FN\"])\n",
    "\n",
    "# Hiển thị\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Client 2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1s = []\n",
    "\n",
    "y_pred = []\n",
    "Iterations = [f\"Iteration {index+1}\" for index in range(5)]\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "for X_batch, y_batch in test_gens[2].take(num_batch_test_eachClient[2]):\n",
    "    X_test.append(X_batch.numpy())\n",
    "    y_test.append(y_batch.numpy())\n",
    "X_test = np.concatenate(X_test, axis=0)\n",
    "y_test = np.concatenate(y_test, axis=0)\n",
    "\n",
    "for iteration in range(len(Iterations)):\n",
    "    y_pred_pre = models['client_2'][iteration].predict(X_test, verbose=1)\n",
    "    y_pred = (y_pred_pre > 0.5).astype(int).flatten()\n",
    "    \n",
    "    precisions.append(precision_score(y_test, y_pred, average='binary'))\n",
    "    recalls.append(recall_score(y_test, y_pred, average='binary'))\n",
    "    f1s.append(f1_score(y_test, y_pred, average='binary'))\n",
    "    \n",
    "print(\"Precision Score: \", precisions)\n",
    "print(\"Recall Score: \", recalls)\n",
    "print(\"F1 Score: \", f1s)  \n",
    "\n",
    "\n",
    "# Vẽ biểu đồ\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(Iterations, precisions, marker='o', linestyle='-', label=\"Precision\", color='blue')\n",
    "plt.plot(Iterations, recalls, marker='s', linestyle='-', label=\"Recall\", color='red')\n",
    "plt.plot(Iterations, f1s, marker='^', linestyle='-', label=\"F1-score\", color='green')\n",
    "\n",
    "# Thêm tiêu đề và nhãn\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Precision, Recall, F1-score over different iterations _ CLIENT 2\")\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)  # Giới hạn từ 0 đến 1\n",
    "plt.grid(True)\n",
    "\n",
    "# Hiển thị đồ thị\n",
    "plt.show()\n",
    "\n",
    "\n",
    "attack_types = ['BenignTraffic', 'DoS&DDoS']\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "    # Vẽ heatmap\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=attack_types, yticklabels=attack_types)\n",
    "plt.yticks(rotation=360)\n",
    "# Thêm nhãn\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix CLIENT 2\")\n",
    "\n",
    "# Hiển thị\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# attack_types = ['BenignTraffic', 'DDoS-ICMP_Flood', 'DDoS-PSHACK_Flood', 'DDoS-RSTFINFlood', 'DDoS-SYN_Flood', \n",
    "#                    'DDoS-SynonymousIP_Flood', 'DDoS-TCP_Flood', 'DDoS-UDP_Flood', 'DoS-SYN_Flood', 'DoS-TCP_Flood', 'DoS-UDP_Flood']\n",
    "\n",
    "metrics = []\n",
    "\n",
    "# Số lượng lớp (10 lớp)\n",
    "num_classes = len(attack_types)\n",
    "\n",
    "# Duyệt từng lớp để tính TP, FP, TN, FN\n",
    "for i in range(num_classes):\n",
    "    TP = cm[i, i]\n",
    "    FP = cm[:, i].sum() - TP\n",
    "    FN = cm[i, :].sum() - TP\n",
    "    TN = cm.sum() - (TP + FP + FN)\n",
    "    \n",
    "    metrics.append([attack_types[i], TP, FP, TN, FN])\n",
    "\n",
    "# Chuyển thành DataFrame\n",
    "df_metrics = pd.DataFrame(metrics, columns=[\"Attack_Types\", \"TP\", \"FP\", \"TN\", \"FN\"])\n",
    "\n",
    "# Vẽ biểu đồ\n",
    "df_metrics.set_index(\"Attack_Types\").plot(kind=\"bar\", figsize=(12, 5), colormap=\"viridis\")\n",
    "\n",
    "# Thêm nhãn\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"TP, FP, TN, FN for Each Class CLIENT 2\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend([\"TP\", \"FP\", \"TN\", \"FN\"])\n",
    "\n",
    "# Hiển thị\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Client_0 \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "file_dir= \"log/client_0_log/14h24p__15-04/\"\n",
    "file_names = [f\"Iteration_{index+1}.csv\" for index in range(5)]\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "accuracy= []\n",
    "loss = []\n",
    "epochs = [\"Epoch_\"+str(i) for i in range(5)]\n",
    "for i, file_name in enumerate(file_names):\n",
    "    # Đọc file CSV\n",
    "    df = pd.read_csv(file_dir + file_name)  # Đổi tên file nếu cần\n",
    "\n",
    "    # Chuyển \"NA\" thành NaN và xử lý nếu cần\n",
    "    df.replace(\"NA\", None, inplace=True)\n",
    "\n",
    "      # Đảm bảo epoch là số nguyên\n",
    "    df[\"accuracy\"] = df[\"accuracy\"].astype(float)\n",
    "    df[\"loss\"] = df[\"loss\"].astype(float)\n",
    "    accuracy.append(df[\"accuracy\"])\n",
    "    loss.append(df[\"loss\"])\n",
    "    print(\"Iteration \"+str(i)+\": \")\n",
    "    for epoch_index,  epoch in enumerate(epochs):\n",
    "        print(epoch + f\": Accuracy: {df['accuracy'][epoch_index]} \\t Loss: {df['loss'][epoch_index]}\")\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "# Vẽ Accuracy\n",
    "plt.subplot(1,2,1)\n",
    "for i in range(3):\n",
    "    plt.plot(epochs, accuracy[i], marker=\"o\", linestyle=\"-\", label=f\"Iteration {i+1}\")\n",
    "    \n",
    "plt.title(\"Accuracy over Epochs Client 1\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Vẽ Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "for i in range(3):\n",
    "    plt.plot(epochs, loss[i], marker=\"s\", linestyle=\"-\", label=f\"Iteration {i+1}\")\n",
    "\n",
    "plt.title(\"Loss over Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Hiển thị biểu đồ\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Client_1\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "file_dir= \"log/client_1_log/14h24p__15-04/\"\n",
    "file_names = [f\"Iteration_{index+1}.csv\" for index in range(5)]\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "accuracy= []\n",
    "loss = []\n",
    "epochs = [\"Epoch_\"+str(i) for i in range(5)]\n",
    "for i, file_name in enumerate(file_names):\n",
    "    # Đọc file CSV\n",
    "    df = pd.read_csv(file_dir + file_name)  # Đổi tên file nếu cần\n",
    "\n",
    "    # Chuyển \"NA\" thành NaN và xử lý nếu cần\n",
    "    df.replace(\"NA\", None, inplace=True)\n",
    "\n",
    "      # Đảm bảo epoch là số nguyên\n",
    "    df[\"accuracy\"] = df[\"accuracy\"].astype(float)\n",
    "    df[\"loss\"] = df[\"loss\"].astype(float)\n",
    "    accuracy.append(df[\"accuracy\"])\n",
    "    loss.append(df[\"loss\"])\n",
    "    print(\"Iteration \"+str(i)+\": \")\n",
    "    for epoch_index,  epoch in enumerate(epochs):\n",
    "        print(epoch + f\": Accuracy: {df['accuracy'][epoch_index]} \\t Loss: {df['loss'][epoch_index]}\")\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "# Vẽ Accuracy\n",
    "plt.subplot(1,2,1)\n",
    "for i in range(3):\n",
    "    plt.plot(epochs, accuracy[i], marker=\"o\", linestyle=\"-\", label=f\"Iteration {i+1}\")\n",
    "    \n",
    "plt.title(\"Accuracy over Epochs Client 1\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Vẽ Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "for i in range(3):\n",
    "    plt.plot(epochs, loss[i], marker=\"s\", linestyle=\"-\", label=f\"Iteration {i+1}\")\n",
    "\n",
    "plt.title(\"Loss over Epochs Client 1\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Hiển thị biểu đồ\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Client_2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "file_dir= \"log/client_2_log/14h24p__15-04/\"\n",
    "file_names = [f\"Iteration_{index+1}.csv\" for index in range(5)]\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "accuracy= []\n",
    "loss = []\n",
    "epochs = [\"Epoch_\"+str(i) for i in range(5)]\n",
    "for i, file_name in enumerate(file_names):\n",
    "    # Đọc file CSV\n",
    "    df = pd.read_csv(file_dir + file_name)  # Đổi tên file nếu cần\n",
    "\n",
    "    # Chuyển \"NA\" thành NaN và xử lý nếu cần\n",
    "    df.replace(\"NA\", None, inplace=True)\n",
    "\n",
    "      # Đảm bảo epoch là số nguyên\n",
    "    df[\"accuracy\"] = df[\"accuracy\"].astype(float)\n",
    "    df[\"loss\"] = df[\"loss\"].astype(float)\n",
    "    accuracy.append(df[\"accuracy\"])\n",
    "    loss.append(df[\"loss\"])\n",
    "    print(\"Iteration \"+str(i)+\": \")\n",
    "    for epoch_index,  epoch in enumerate(epochs):\n",
    "        print(epoch + f\": Accuracy: {df['accuracy'][epoch_index]} \\t Loss: {df['loss'][epoch_index]}\")\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "# Vẽ Accuracy\n",
    "plt.subplot(1,2,1)\n",
    "for i in range(3):\n",
    "    plt.plot(epochs, accuracy[i], marker=\"o\", linestyle=\"-\", label=f\"Iteration {i+1}\")\n",
    "    \n",
    "plt.title(\"Accuracy over Epochs _Client 2\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Vẽ Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "for i in range(3):\n",
    "    plt.plot(epochs, loss[i], marker=\"s\", linestyle=\"-\", label=f\"Iteration {i+1}\")\n",
    "\n",
    "plt.title(\"Loss over Epochs Client 2\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Hiển thị biểu đồ\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
