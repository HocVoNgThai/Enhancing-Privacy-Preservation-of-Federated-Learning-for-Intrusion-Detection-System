{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f532a9bb-0cbe-49bb-b22f-155e520aaaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import dask.dataframe as dk\n",
    "import tensorflow as tf\n",
    "import io\n",
    "from tensorflow.keras.utils import Sequence, to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Tắt một số cảnh báo không cần thiết từ TensorFlow\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.config.optimizer.set_jit(False)  # Tắt XLA để tránh lỗi CUDA\n",
    "\n",
    "input_files = [f\"file{i+1}.csv\" for i in range(3)]\n",
    "temp_dir = \"FL_Data1/\"\n",
    "input_files = [temp_dir + output_file for output_file in input_files]\n",
    "print(input_files)\n",
    "\n",
    "df = [dk.read_csv(file) for file in input_files]\n",
    "print(df[1].dtypes)\n",
    "\n",
    "batch_size = 512\n",
    "ratio_test_all = 0.2\n",
    "features_len = len(df[1].columns) - 1\n",
    "print(\"Feature Len: \", features_len)\n",
    "\n",
    "def standardize_dask_df(dask_df):\n",
    "    scaler = StandardScaler()\n",
    "    X = dask_df.drop(columns=['label']).compute().values\n",
    "    scaler.fit(X)\n",
    "    for part in dask_df.to_delayed():\n",
    "        part = part.compute()\n",
    "        if part.empty:\n",
    "            continue\n",
    "        X_part = part.drop(columns=['label']).values\n",
    "        X_scaled = scaler.transform(X_part)\n",
    "        part.iloc[:, :-1] = X_scaled\n",
    "        yield part\n",
    "\n",
    "scaled_dfs = []\n",
    "for dask_df in df:\n",
    "    scaled_parts = list(standardize_dask_df(dask_df))\n",
    "    scaled_df = dk.from_pandas(pd.concat(scaled_parts), npartitions=dask_df.npartitions)\n",
    "    scaled_dfs.append(scaled_df)\n",
    "\n",
    "train_dfs = []\n",
    "val_dfs = []\n",
    "test_dfs = []\n",
    "for dff in scaled_dfs:\n",
    "    train_df, val_test_df = dff.random_split([1 - ratio_test_all, ratio_test_all])\n",
    "    test_df, val_df = val_test_df.random_split([1 - 0.25, 0.25])\n",
    "    train_dfs.append(train_df)\n",
    "    val_dfs.append(val_df)\n",
    "    test_dfs.append(test_df)\n",
    "\n",
    "def dask_to_tf_dataset(dask_df, batch_size):\n",
    "    def generator():\n",
    "        for batch in dask_df.to_delayed():\n",
    "            batch = batch.compute()\n",
    "            if batch.empty:\n",
    "                continue\n",
    "            X = batch.drop(columns='label').values.astype(np.float32)\n",
    "            y = batch['label'].values.astype(np.int32)\n",
    "            num_splits = max(1, len(X) // batch_size)\n",
    "            X_batches = np.array_split(X, num_splits)\n",
    "            y_batches = np.array_split(y, num_splits)\n",
    "            for X_batch, y_batch in zip(X_batches, y_batches):\n",
    "                yield X_batch, y_batch\n",
    "    output_signature = (\n",
    "        tf.TensorSpec(shape=(None, features_len), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
    "    )\n",
    "    return tf.data.Dataset.from_generator(generator, output_signature=output_signature).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_gens = [dask_to_tf_dataset(train_df, batch_size).repeat() for train_df in train_dfs]\n",
    "val_gens = [dask_to_tf_dataset(val_df, batch_size).repeat() for val_df in val_dfs]\n",
    "test_gens = [dask_to_tf_dataset(test_df, batch_size).repeat() for test_df in test_dfs]\n",
    "\n",
    "from server_no_HE import Server\n",
    "from client_no_HE import Client\n",
    "import datetime\n",
    "import tenseal as ts\n",
    "\n",
    "num_servers = 1\n",
    "num_clients = 3\n",
    "\n",
    "stepsPerEpoch_Clients = [int(np.ceil(train_dfs[index].shape[0].compute()) / batch_size) for index in range(num_clients)]\n",
    "stepsValidate_Clients = [int(np.ceil(val_dfs[index].shape[0].compute()) / batch_size) for index in range(num_clients)]\n",
    "stepsTest_Clients = [int(np.ceil(test_dfs[index].shape[0].compute()) / batch_size) for index in range(num_clients)]\n",
    "\n",
    "active_servers_list = ['server_' + str(i) for i in range(num_servers)]\n",
    "active_clients_list = ['client_' + str(i) for i in range(num_clients)]\n",
    "print(active_servers_list)\n",
    "print(active_clients_list)\n",
    "\n",
    "def init_he_context():\n",
    "    context = ts.context(\n",
    "        ts.SCHEME_TYPE.CKKS,\n",
    "        poly_modulus_degree=16384,\n",
    "        coeff_mod_bit_sizes=[60, 40, 40, 40, 40, 60]\n",
    "    )\n",
    "    context.generate_galois_keys()\n",
    "    context.global_scale = 2**40\n",
    "    return context\n",
    "\n",
    "context = init_he_context()\n",
    "\n",
    "agents_dict = {}\n",
    "serverObjects = {server_name: Server(server_name=server_name, active_clients_list=active_clients_list)\n",
    "                 for server_name in active_servers_list}\n",
    "\n",
    "clientObjects = {client_name: Client(client_name, train_gens[clientID], val_gens[clientID], test_gens[clientID],\n",
    "                                     stepsPerEpoch_Clients[clientID], stepsValidate_Clients[clientID], stepsTest_Clients[clientID],\n",
    "                                     active_clients_list=active_clients_list, he_context=context)\n",
    "                 for clientID, client_name in enumerate(active_clients_list)}\n",
    "\n",
    "for index, client_name in enumerate(active_clients_list):\n",
    "    clientObjects[client_name].get_steps_per_epoch()\n",
    "    clientObjects[client_name].get_validation_steps()\n",
    "    clientObjects[client_name].get_test_steps()\n",
    "\n",
    "agents_dict['server'] = serverObjects\n",
    "agents_dict['client'] = clientObjects\n",
    "\n",
    "for agent_name, agent in serverObjects.items():\n",
    "    agent.set_agentsDict(agents_dict=agents_dict)\n",
    "for agent_name, agent in clientObjects.items():\n",
    "    agent.set_agentsDict(agents_dict=agents_dict)\n",
    "\n",
    "# Giải phóng DataFrame sau khi không cần thiết\n",
    "del train_dfs, val_dfs, test_dfs\n",
    "\n",
    "server = agents_dict['server']['server_0']\n",
    "if __name__ == '__main__':\n",
    "    server.InitLoop()\n",
    "    server.final_statistics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
